{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN-Transformer Model for Financial Time Series Forecasting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Time series forecasting plays a critical role in many domains, but it is especially challenging in financial markets due to complex short-term and long-term patterns&#8203;:contentReference[oaicite:0]{index=0}. A recent study by Zeng *et al.* (2023) addressed this challenge by combining Convolutional Neural Networks (CNNs) with Transformer networks to improve stock price prediction&#8203;:contentReference[oaicite:1]{index=1}. This hybrid approach, termed **CNN-Transformer Time Series model (CTTS)**, aims to leverage CNNs for local feature extraction and Transformers for capturing long-range dependencies within the data&#8203;:contentReference[oaicite:2]{index=2}&#8203;:contentReference[oaicite:3]{index=3}.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methodology\n",
        "The **CTTS model** first uses a one-dimensional CNN to extract local temporal features from the input sequence. The CNN slides a window over the time series, and each window is processed to produce an embedding vector (a \"token\") that captures short-term patterns&#8203;:contentReference[oaicite:4]{index=4}. Positional embeddings are then added to these tokens to retain temporal order information&#8203;:contentReference[oaicite:5]{index=5}. \n",
        "\n",
        "Next, the sequence of token embeddings is fed into a Transformer encoder. The Transformer layer learns the **long-term dependencies** between distant time points through self-attention mechanisms&#8203;:contentReference[oaicite:6]{index=6}. By combining the CNN and Transformer, the model can understand both recent fluctuations and broader trends. Finally, the Transformer’s output (a learned representation of the entire sequence) is passed to a feed-forward network (MLP) with a softmax layer to predict the future trend&#8203;:contentReference[oaicite:7]{index=7}. The model outputs a probability for each class (stock price going **up**, **down**, or **flat**), enabling it to forecast the direction of the next time step’s price movement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data and Experimental Setup\n",
        "The model was evaluated on intraday stock market data from the S&P 500 index&#8203;:contentReference[oaicite:8]{index=8}. Specifically, the dataset consisted of one-minute interval stock prices for all S&P 500 constituent companies throughout the year 2019&#8203;:contentReference[oaicite:9]{index=9}. The first 39 weeks of 2019 were used for training (with a portion for validation), and the remaining 13 weeks (the last quarter of 2019) served as the test set&#8203;:contentReference[oaicite:10]{index=10}. \n",
        "\n",
        "For each stock, the historical price time series was segmented into samples of 80 time steps (80 minutes) as model inputs, with the goal to predict the price movement at the 81st minute&#8203;:contentReference[oaicite:11]{index=11}. This formulation translates the forecasting task into a classification problem — determining whether the price will go up, down, or remain unchanged in the next minute, given the recent 80-minute history.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and Comparison\n",
        "The CNN-Transformer model (CTTS) achieved superior forecasting performance compared to both classical statistical models and other deep learning models&#8203;:contentReference[oaicite:12]{index=12}. Notably, it was benchmarked against:\n",
        "- **ARIMA** (Autoregressive Integrated Moving Average), a traditional statistical forecasting method,\n",
        "- **EMA** (Exponential Moving Average) smoothing,\n",
        "- **DeepAR**, a state-of-the-art recurrent neural network (RNN/LSTM-based) forecasting model&#8203;:contentReference[oaicite:13]{index=13}.\n",
        "\n",
        "CTTS outperformed all these baselines in predicting stock price direction. For example, in the three-class classification (up/down/flat), CTTS attained about **44.1%** accuracy, whereas the best baseline (EMA) reached only around **39.5%**&#8203;:contentReference[oaicite:14]{index=14}. In the two-class case (predicting up vs. down-or-flat), CTTS achieved about **56.7%** accuracy versus ~53% by the best baseline&#8203;:contentReference[oaicite:15]{index=15}. The table below summarizes the accuracy results:\n",
        "\n",
        "| Model    | 2-Class Accuracy | 2-Class* (Thresholded) | 3-Class Accuracy | 3-Class* (Thresholded) |\n",
        "|----------|------------------|------------------------|------------------|------------------------|\n",
        "| ARIMA    | 50.9%           | 51.8%                 | 37.5%           | 38.4%                 |\n",
        "| EMA      | 53.2%           | 59.9%                 | 39.5%           | 41.7%                 |\n",
        "| DeepAR   | 51.1%           | 53.6%                 | 37.4%           | 38.7%                 |\n",
        "| **CTTS** | **56.7%**       | **66.8%**             | **44.1%**       | **55.2%**             |\n",
        "\n",
        "*Table: Direction prediction accuracy on the test set (last quarter of 2019). 2-Class denotes up vs. not-up, and 3-Class denotes up vs. down vs. flat. \"*Thresholded*\" columns (2-Class* and 3-Class*) show accuracy when only high-confidence predictions (those above a probability threshold) are considered&#8203;:contentReference[oaicite:16]{index=16}.*\n",
        "\n",
        "These results demonstrate a clear advantage for the CNN-Transformer approach. In particular, the CTTS model’s predictions were not only more accurate overall, but also **more calibrated**. When the model was very confident in a prediction (as reflected by a high predicted probability), it was usually correct&#8203;:contentReference[oaicite:17]{index=17}. By filtering for high-confidence forecasts (the thresholded evaluation), the accuracy of CTTS rose substantially (e.g. reaching **55.2%** in the 3-class task), widening the gap over the other methods&#8203;:contentReference[oaicite:18]{index=18}&#8203;:contentReference[oaicite:19]{index=19}.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion and Real-World Impact\n",
        "The study illustrates that **Convolutional Neural Networks can be effectively applied to time-series forecasting** beyond their traditional image-processing domain. By integrating CNNs with Transformers, the researchers captured short-term fluctuations and long-term trends simultaneously, leading to superior performance in a challenging financial forecasting task&#8203;:contentReference[oaicite:20]{index=20}&#8203;:contentReference[oaicite:21]{index=21}. The CNN-Transformer model (CTTS) significantly outperformed a classical ARIMA model and an LSTM-based model on stock price data&#8203;:contentReference[oaicite:22]{index=22}, highlighting the potential of CNN-based architectures in real-world forecasting.\n",
        "\n",
        "Moreover, the reliability of the model’s probability outputs (as evidenced by the thresholded accuracy gains) is important for decision-making. In a financial context, this means traders or analysts could choose to act only on the model’s predictions when it is most confident, potentially improving the success rate of trades&#8203;:contentReference[oaicite:23]{index=23}. The authors noted that the CTTS approach could inform practical trading strategies — for instance, buying, selling, or holding a stock based on the predicted movement and its confidence level&#8203;:contentReference[oaicite:24]{index=24}.\n",
        "\n",
        "In summary, this research demonstrates a high-utility application of CNNs in time series forecasting. The hybrid CNN-Transformer methodology provided a robust framework for financial time series prediction, and similar concepts could be extended to other domains like energy demand or weather forecasting where multi-scale temporal patterns are present.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
